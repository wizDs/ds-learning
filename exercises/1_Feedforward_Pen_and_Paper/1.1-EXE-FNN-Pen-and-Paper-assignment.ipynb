{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7yQ-xkp8Ed_"
   },
   "source": [
    "# 02456 Deep Learning Exercise 1 Pen and Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise a)\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "y_j & = h_2(a_j^{(2)}) \\\\\n",
    "\n",
    "& = h_2(\\sum_{i}^M w^{(2)}_{ji} z^{(1)}_i) \\\\\n",
    "\n",
    "& = h_2(\\sum_{i}^M w^{(2)}_{ji} h_1(a^{(1)}_i )) \\\\\n",
    "\n",
    "& = h_2(\\sum_{i}^M w^{(2)}_{ji} h_1(\\sum_{d=0}^D w^{(1)}_{id} x_d)) \\\\\n",
    "\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise b)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "y_j & = h_3(a_j^{(3)})\\\\\n",
    "\n",
    "& = h_3(\\sum_{i=0}^M w^{(3)}_{ji} z^{(2)}_i) \\\\\n",
    "\n",
    "& = h_3(\\sum_{i=0}^M w^{(3)}_{ji} h_2(a^{(2)}_i )) \\\\\n",
    "\n",
    "& = h_3(\\sum_{i=0}^M w^{(3)}_{ji} h_2(\\sum_{d=0}^D w^{(2)}_{id} z_d)) \\\\\n",
    "\n",
    "& = h_3(\\sum_{i=0}^M w^{(3)}_{ji} h_2(\\sum_{d=0}^D w^{(2)}_{id} h_1(a^{(1)}_d ))) \\\\\n",
    "\n",
    "& = h_3(\\sum_{i=0}^M w^{(3)}_{ji} h_2(\\sum_{d=0}^D w^{(2)}_{id} h_1(\\sum_{s=0}^S w^{(1)}_{ds} x_s ))) \\\\\n",
    "\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise c)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j & = h_L(a_j^{(L)}) & \\\\\n",
    "z^{(l)}_j & = h_l(a_j^{(l)}) & l=1,\\ldots,L \\\\\n",
    "a^{(l)}_j & = \\sum_{i} w^{(l)}_{ji} z^{(l-1)}_i &  l=2,\\ldots,L \\\\\n",
    "a^{(1)}_j & = \\sum_{i} w^{(1)}_{ji} x_i & \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise e)\n",
    "\n",
    "Let $\\mathcal{P} = p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w)$ and we then simplify the following\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "\\mathcal{P} & =\n",
    "\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w)\\\\\n",
    "\n",
    "& =\\prod_{n=1}^N \\mathcal{N}(\\mathbf{t}_n|\\mathbf{y}(\\mathbf{x}_n),\\sigma^2 \\mathbf{I})\\\\\n",
    "\n",
    "& =\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )\\\\\n",
    "\n",
    "& =\\bigg(\\frac{1}{\\sqrt{2\\pi \\sigma^2}^D}\\bigg)^N \\prod_{n=1}^N\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )\\\\\n",
    "\n",
    "& =\\frac{1}{\\sqrt{2\\pi \\sigma^2}^{ND}} \\prod_{n=1}^N\n",
    "\\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )\\\\\n",
    "\n",
    "\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Apply the natural log and multiply with -1 and then\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "-log\\ \\mathcal{P}  & = -log \\big( \\frac{1}{\\sqrt{2\\pi \\sigma^2}^{ND}} \\prod_{n=1}^N \\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )  \\big)\\\\\n",
    "\n",
    "& = -\\big[log \\big( \\frac{1}{\\sqrt{2\\pi \\sigma^2}^{ND}} \\big) + log \\big( \\prod_{n=1}^N \\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )  \\big)\\big]\\\\\n",
    "\n",
    "& = -\\big[ log(1) - log(\\sqrt{2\\pi \\sigma^2}^{ND}) + log \\big( \\prod_{n=1}^N \\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )  \\big)\\big]\\\\\n",
    "\n",
    "& = -\\big[ - log(\\sqrt{2\\pi \\sigma^2}^{ND}) + log \\big( \\prod_{n=1}^N \\exp ( - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 )  \\big)\\big]\\\\\n",
    "\n",
    "& = -\\bigg[ - log(\\sqrt{2\\pi \\sigma^2}^{ND}) + \\sum_{n=1}^N \\big[  - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 \\big]   \\bigg]\\\\\n",
    "\n",
    "& = log(\\sqrt{2\\pi \\sigma^2}^{ND}) - \\sum_{n=1}^N \\big[  - || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 /2\\sigma^2 \\big] \\\\\n",
    "\n",
    "\n",
    "& = log(\\sqrt{2\\pi \\sigma^2}^{ND}) +\\frac{1}{2\\sigma^2} \\sum_{n=1}^N   || \\mathbf{y}(\\mathbf{x}_n) - \\mathbf{t}_n||_2^2 \\\\\n",
    "\n",
    "& = log(\\sqrt{2\\pi \\sigma^2}^{ND}) +\\frac{1}{2\\sigma^2} E(w)   \\\\\n",
    "\n",
    "\n",
    "& = log((2\\pi \\sigma^2)^{ND/2}) +\\frac{1}{2\\sigma^2} E(w)   \\\\\n",
    "\n",
    "& = \\frac{ND}{2} log(2\\pi \\sigma^2) +\\frac{1}{2\\sigma^2} E(w)   \\\\\n",
    "\n",
    "& = \\frac{ND}{2} log~2\\pi \\sigma^2 +\\frac{1}{2\\sigma^2} E(w)   \\\\\n",
    "\n",
    "\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) to (10) follows from https://study.com/skill/learn/expanding-a-logarithmic-expression-with-square-roots-explanation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying log, we can transform the multiplication term into a summation term for computational reasons and by multiply with -1, the likelihood function is reversed, so instead of maximizing the likelihood function, we can minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise g) \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{t}_n|\\mathbf{x}_n,w) & = \\prod_{k=1}^K \\left[ y_k(\\mathbf{x}_n)\\right]^{t_{nk}} \\\\\n",
    "\n",
    "& = \\prod_{k=1}^K \\left[ \\frac{\\exp ( x_k )}{\\sum_j \\exp ( x_j )}\\right]^{t_{nk}} \\\\\n",
    "\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{P} = p(\\mathbf{t}_1,\\ldots,\\mathbf{t}_N|\\mathbf{x}_1,\\ldots,\\mathbf{x}_N,w)$ and we can simplify the follow the same procedure\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "E(w) = -log~\\mathcal{P} & = -log\\left(\\prod_{n=1}^N p(\\mathbf{t}_n|\\mathbf{x}_n,w) \\right)\\\\\n",
    "\n",
    "& =-log\\left(\\prod_{n=1}^N \\prod_{k=1}^K \\left[ y_k(x_n)\\right]^{t_{nk}} \\right)\\\\\n",
    "\n",
    "& =-\\left(\\sum_{n=1}^N \\sum_{k=1}^K log\\left[ y_k(x_n)^{t_{nk}}\\right] \\right)\\\\\n",
    "\n",
    "& =-\\left(\\sum_{n=1}^N \\sum_{k=1}^K  t_{nk} \\cdot log\\left[ y_k(x_n)\\right] \\right)\\\\\n",
    "\n",
    "& =-\\sum_{n=1}^N \\sum_{k=1}^K  t_{nk} \\cdot log~ y_k(x_n)\\\\\n",
    "\n",
    "\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "to get cross entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exercise g) we found that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "E(w) & = -\\sum_{n=1}^N \\sum_{k=1}^K  t_{nk} \\cdot log~ y_k(x_n)\\\\\n",
    "\n",
    "& = - \\sum_{k=1}^K \\sum_{n=1}^N t_{nk} \\cdot log~ y_k(x_n)\\\\\n",
    "\n",
    "& = - \\sum_{k=1}^K t_{k} \\cdot log~ y_k(x_n)\\\\\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\sum_{n=1}^N t_{nk} = t_{k}$ as we assume that our training set consists of one example so we can drop the training point index $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since $y_k= \\frac{\\exp ( a_k )}{\\sum_j \\exp ( a_j )}$ we can simplify:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "E(w) & =  -\\sum_{k=1}^K  t_{k} \\cdot log\\big[ \\frac{\\exp ( a^{(L)}_k )}{\\sum_j \\exp ( a^{(L)}_j )} \\big]\\\\\n",
    "\n",
    "& = -\\sum_{k=1}^K  t_{k} \\cdot \\bigg[a^{(L)}_k-log~\\sum_j \\exp ( a^{(L)}_j )\\bigg]\\\\\n",
    "\n",
    "& =  - \\sum_{k=1}^K t_{k} a^{(L)}_k + \\sum_{k=1}^K t_{k}\\cdot log\\left(\\sum_j \\exp ( a^{(L)}_j )\\right)\\\\\n",
    "\n",
    "\n",
    "& =  - \\sum_{k=1}^K t_{k} a^{(L)}_k + log\\left(\\sum_j \\exp ( a^{(L)}_j )\\right)\\\\\n",
    "\n",
    "& =  - \\sum_{k=1}^K t_{k} a^{(L)}_k + log~\\sum_{j} \\exp ( a^{(L)}_j )\\\\\n",
    "\n",
    "\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that from (3) to (4) we see that $log\\left(\\sum_j \\exp ( a^{(L)}_j )\\right)$ is independent of k and $t_k$ is a one hot encoded variable and thus only 1 when equal to k.\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "\\frac{\\partial }{\\partial a_j^{(L)}} E(w)& = \\frac{\\partial }{\\partial a_j^{(L)}} \\left[ - \\sum_{k=1}^K t_{k} a^{(L)}_k + log~\\sum_{j} \\exp ( a^{(L)}_j )\\right]\\\\\n",
    "\n",
    "& = \\frac{\\partial }{\\partial a_j^{(L)}} \\left[  log~\\sum_{j} \\exp ( a^{(L)}_j )\\right]\\\\\n",
    "\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that from (1) to (2) we can ignore the first term as it is not dependent on $a_j^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let $f(g(x_j)) = log(\\sum_j exp(x_j))$, where $f(g) = log(g)$, $g(x_j) = \\sum_j exp(x_j)$ and $x_j=a_j^{(L)}$ then we can rewrite:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "\\delta_j^{(L)}= \\frac{\\partial }{\\partial a_j^{(L)}} E(w) & = \\frac{\\partial }{\\partial x_j} \\left[  log~\\sum_{j} \\exp ( x_j )\\right]\\\\\n",
    "\n",
    "& = \\frac{\\partial }{\\partial x_j}   f\\left[ g ( x_j )\\right]\\\\\n",
    "\n",
    "& = \\left. \\frac{\\partial f(g)}{\\partial g}\\right|_{g=g(w)}    \\cdot \\frac{\\partial g ( x_j )}{\\partial x_j}   \\\\\n",
    "\n",
    "& = \\frac{\\partial}{\\partial g}\\left[ log(g)\\right]    \\cdot \\frac{\\partial }{\\partial x_j}\\left[\\sum_{j} \\exp ( x_j )\\right]   \\\\\n",
    "\n",
    "& = \\frac{1}{g}    \\cdot \\exp ( x_j ) \\\\\n",
    "\n",
    "& = \\frac{1}{\\sum_{j} \\exp ( x_j )}    \\cdot \\exp ( x_j )  \\\\\n",
    "\n",
    "& = \\frac{\\exp ( x_j )}{\\sum_{j} \\exp ( x_j )}  \\\\\n",
    "\n",
    "& = \\frac{\\exp ( a_j^{(L)} )}{\\sum_{j} \\exp ( a_j^{(L)} )}  \\\\\n",
    "\n",
    "& = y_j  \\\\\n",
    "\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus it is shown that $\\frac{\\partial }{\\partial a_j^{(L)}} E(w) = y_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering the gradient for layer $l$, $\\frac{\\partial }{\\partial w_{ji}^{(l)}} E(w) = \\delta_j^{(l)} z_i^{(l-1)}$ with $\\delta^{(l)}_j = \\frac{\\partial E(w)}{\\partial a^{(l)}_{j}} $, we see that each $\\delta_j^{(l)}$, the $\\delta_j^{(l+1)}$ and weights can be reused for computations, making it more efficient to make backpropagation. The rule makes a lot of sense as it is a layered model and each $\\delta$ feeds recursively into the next layer of the computation, which can be seen in for general back propagation rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise l)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E(w)}{\\partial w^{(l)}_{ji}} & = \\delta^{(l)}_j  z^{(l-1)}_i \\\\\n",
    "\\delta^{(l)}_j & = \\sum_{k=1}^K  \\sum_{n=1}^N  \\delta^{(l+1)}_{nk}  w^{(l+1)}_{kj} h_{l}'(a^{(l)}_{nj}) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_1_FNN_Pen_and_Paper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
